{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# AI\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Natural Language Processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from fuzzywuzzy import fuzz\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatgpt api\n",
    "load_dotenv('keys.env')\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# this function call the openai api, input the prompt and return the response in a string\n",
    "# string is cleaned to only return the python dictionary\n",
    "def transform(row, prompt):\n",
    "    # concat directions and the input text\n",
    "    prompt = f'{prompt}{row}'\n",
    "    # print(prompt)\n",
    "    retry = 0\n",
    "\n",
    "    # call api\n",
    "    while retry <= 10:\n",
    "        print(f\"Start API call at {datetime.now()}\")\n",
    "        try:\n",
    "            # Call API\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                timeout=60\n",
    "            )\n",
    "            content = str(response.choices[0].message.content)\n",
    "\n",
    "            # Handle 'null' and 'None' by replacing them with actual Python None\n",
    "            content = content.replace(\"null\", \"None\")\n",
    "\n",
    "            # Extract JSON-like content between first `{` and last `}`\n",
    "            start = content.find('{')\n",
    "            end = content.rfind('}')\n",
    "            if start != -1 and end != -1:\n",
    "                content = content[start:end+1]\n",
    "                print(f\"retrieved dictionary at {datetime.now()}\")\n",
    "                return content\n",
    "\n",
    "            else:\n",
    "                print(f\"No valid JSON-like content found in response at {datetime.now()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during API call or processing: {e}\")\n",
    "        \n",
    "        retry += 1\n",
    "        print(f\"Retrying {retry}...\")\n",
    "        time.sleep(4)\n",
    "\n",
    "    print(\"Failed to retrieve dictionary after maximum retries.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_plot(ax, title='', xlabel='', ylabel='', legend_title='', rotation=60):\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    if xlabel:\n",
    "        ax.set_xlabel(xlabel)\n",
    "    if ylabel:    \n",
    "        ax.set_ylabel(ylabel)\n",
    "    if legend_title:\n",
    "        ax.legend_.set_title(legend_title)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=rotation, ha='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_eval(row):\n",
    "    if isinstance(row, str):\n",
    "        return ast.literal_eval(row)\n",
    "    return row\n",
    "\n",
    "# Convert dictionary column into new DataFrame columns\n",
    "def dict_to_cols(df, dict_column):\n",
    "    # Safely evaluate the dictionary column\n",
    "    df[dict_column] = df[dict_column].apply(safe_eval)\n",
    "    \n",
    "    # Normalize the dictionary column into separate columns\n",
    "    normalized_df = pd.json_normalize(df[dict_column])\n",
    "    \n",
    "    # Combine the normalized columns with the original DataFrame (optional)\n",
    "    return normalized_df\n",
    "\n",
    "# this function check if the text has any non-English characters\n",
    "def contains_non_english(text):\n",
    "    # Check if the string contains any non-ASCII characters\n",
    "    # return pd.notna(s) and not bool(re.fullmatch(r'[\\x00-\\x7F]*', str(s)))\n",
    "    if re.search(r'[^\\x00-\\x7F]+', text):  # Matches any non-ASCII characters\n",
    "        # Check if the text contains characters outside the English alphabet\n",
    "        # Specifically look for Korean, Japanese, or any other script\n",
    "        if re.search(r'[^\\x00-\\x7F\\u0000-\\u007F]+', text):  # Matches non-ASCII characters\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# this function remove non-printable characters and replace non-ASC II characters to ASC II\n",
    "def replace_non_ascii(s):\n",
    "    # remove non-printable characters\n",
    "    s = ''.join(ch for ch in s if ch.isprintable())\n",
    "    # Replace the en dash (–) with a hyphen (-)\n",
    "    s = s.replace(\"–\", \"-\")\n",
    "    # # Replace the ampersand (&) symbol with the word \"and\"\n",
    "    # s = s.replace(\"&\", \"and\")\n",
    "    return s\n",
    "\n",
    "# remove unwanted rows by company names and job titles\n",
    "def refine_rows(df, drop_rows, keep_rows):\n",
    "    # drop not legit companies\n",
    "    df = df[~df['companyName'].isin(drop_rows)]\n",
    "    # keep legit job titles\n",
    "    df = df[df['title'].str.contains('|'.join(keep_title_rows), case=False, na=False)]\n",
    "    # remove duplicates\n",
    "    df = df.drop_duplicates(subset=['companyName', 'title', 'publishedAt'])\n",
    "    return df\n",
    "\n",
    "# condense complex job titles into a few well defined ones\n",
    "def simplify_title(row, title_list):\n",
    "    if isinstance(row['title'], str):  # Ensure it's a string\n",
    "        for title in title_list:\n",
    "            if title.lower() in row['title'].lower():\n",
    "                return title\n",
    "    return None  # Return None if not a match or if title is not a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salary Analysis Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function cleans the salary data for further feature engineering and visualization\n",
    "# zeros are removed\n",
    "# The dataset has a salary column obtained by web scraping, and some salary from AI generated values\n",
    "# fill hourly and yearly salary if a salary column exist, otherwise keep the AI generated values\n",
    "def clean_salary(df, cols):\n",
    "    # remove zeros\n",
    "    df = df.replace(0, None)\n",
    "    # call parse_and_fill_salary to fill the hourly and yearly salary columns\n",
    "    df = df.apply(lambda row: parse_and_fill_salary(row), axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# this function parse the salary column and fill the hourly and yearly salary columns\n",
    "def parse_and_fill_salary(row):\n",
    "    # Function to extract the numeric values from salary strings\n",
    "    def extract_salary_value(salary_str):\n",
    "        numbers = re.findall(r'\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?', salary_str)\n",
    "        # Convert the numbers to float and return the list\n",
    "        return [float(num.replace(',', '')) for num in numbers]\n",
    "\n",
    "    # Checking if 'salary' is defined and hourly or yearly salary is missing\n",
    "    # if row['salary'] is not None:\n",
    "    if isinstance(row['salary'], str):\n",
    "        salary_value = extract_salary_value(row['salary'])\n",
    "        \n",
    "        # Case 1: If salary is hourly and min/max hourly salary is missing\n",
    "        if 'hr' in row['salary'] or 'hour' in row['salary']:\n",
    "            row['min_hourly_salary'] = min(salary_value)\n",
    "            row['max_hourly_salary'] = max(salary_value)\n",
    "            row['min_yearly_salary'] = None\n",
    "            row['max_yearly_salary'] = None\n",
    "        \n",
    "        # Case 2: If salary is monthly and min/max yearly salary is missing\n",
    "        elif 'mo' in row['salary'] or 'month' in row['salary']:\n",
    "            row['min_yearly_salary'] = min(salary_value)*12\n",
    "            row['max_yearly_salary'] = max(salary_value)*12\n",
    "            row['min_hourly_salary'] = None\n",
    "            row['max_hourly_salary'] = None\n",
    "        \n",
    "        # Case 3: If salary is yearly and min/max hourly salary is missing\n",
    "        elif 'yr' in row['salary'] or 'year' in row['salary']:\n",
    "            row['min_yearly_salary'] = min(salary_value)\n",
    "            row['max_yearly_salary'] = max(salary_value)\n",
    "            row['min_hourly_salary'] = None\n",
    "            row['max_hourly_salary'] = None\n",
    "            \n",
    "    return row\n",
    "\n",
    "# calculate average salary\n",
    "def calculate_average_salary(row, col_min, col_max):\n",
    "    # Check if both specified columns are not NaN\n",
    "    if pd.notna(row[col_min]) and pd.notna(row[col_max]):\n",
    "        return (row[col_min] + row[col_max]) / 2\n",
    "    # If the minimum column is NaN, return the maximum column value\n",
    "    elif pd.isna(row[col_min]) and pd.notna(row[col_max]):\n",
    "        return row[col_max]\n",
    "    # If the maximum column is NaN, return the minimum column value\n",
    "    elif pd.isna(row[col_max]) and pd.notna(row[col_min]):\n",
    "        return row[col_min]\n",
    "    else:\n",
    "        # If both are NaN, return NaN (or any custom value you'd prefer)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Skill Analysis Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means clustering\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize, remove stopwords, and lemmatize\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    if not words:\n",
    "        return None\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def KMeans_words(n, df):\n",
    "    df = df.copy()\n",
    "    # df['cleaned_text'] = df.apply(preprocess_text)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(df)\n",
    "\n",
    "    # Fit KMeans clustering model\n",
    "    kmeans = KMeans(n_clusters=n, random_state=42)\n",
    "    df['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "    # grouped_words = df.groupby(['cluster', 'simplified_job_title'])['cleaned_text'].apply(list).reset_index()\n",
    "    # return grouped_words\n",
    "    return df['cluster']\n",
    "\n",
    "def most_frequent_word(text_list):\n",
    "    # Flatten the list of cleaned text into individual words\n",
    "    # all_words = \" \".join(text_list).split()\n",
    "    # Count the frequency of each word\n",
    "    # word_counts = Counter(all_words)\n",
    "    word_counts = Counter(text_list)\n",
    "    # Find the most common word\n",
    "    if word_counts:\n",
    "        return word_counts.most_common(1)[0][0]\n",
    "    return None\n",
    "\n",
    "def get_top_skills(num_clusters, exploded_df, threshold_percentage=50):\n",
    "    # clean the required skills again for clustering\n",
    "    exploded_df = exploded_df[exploded_df['cleaned_required_skills'] != 'etl'].copy()\n",
    "    exploded_df['cleaned_required_skills'] = exploded_df['cleaned_required_skills'].apply(add_qualifier)\n",
    "    exploded_df = exploded_df[exploded_df['cleaned_required_skills'].notna()].copy()\n",
    "    \n",
    "    # us k-means clustring to group similar skill names  \n",
    "    exploded_df['cluster'] = KMeans_words(n=num_clusters, df=exploded_df['cleaned_required_skills'])\n",
    "    \n",
    "    # calculate counts and standard names from the clusters\n",
    "    grouped_skills = exploded_df.groupby('cluster')['cleaned_required_skills'].apply(list).reset_index()\n",
    "    grouped_skills['count'] = grouped_skills['cleaned_required_skills'].apply(len)\n",
    "    grouped_skills['most_freq_skill'] = grouped_skills['cleaned_required_skills'].apply(most_frequent_word)\n",
    "    \n",
    "    # Calculate the frequency of the most frequent word in the cluster\n",
    "    def most_freq_word_count_percentage(skill_list, most_freq_skill):\n",
    "        return skill_list.count(most_freq_skill) / len(skill_list) * 100\n",
    "    \n",
    "    grouped_skills['most_freq_skill_percentage'] = grouped_skills.apply(\n",
    "        lambda row: most_freq_word_count_percentage(row['cleaned_required_skills'], row['most_freq_skill']),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Exclude clusters where the most frequent word count is below the threshold percentage\n",
    "    filtered_skills = grouped_skills[grouped_skills['most_freq_skill_percentage'] >= threshold_percentage]\n",
    "    \n",
    "    # get the top 20 skills\n",
    "    top_skills = filtered_skills.groupby('most_freq_skill')['count'].sum().reset_index()\n",
    "    top_skills = top_skills.sort_values(by='count', ascending=False).head(20)\n",
    "\n",
    "    return top_skills\n",
    "\n",
    "# test and plot 9 different number of clusters\n",
    "def tune_num_clusters(df):\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    l = len(df['cleaned_required_skills'].unique())\n",
    "    list_num_clusters = [int(l/10), int(l/9), int(l/8), int(l/7), int(l/6), int(l/5), int(l/4), int(l/3), int(l/2)]\n",
    "    for ax, k in zip(axes.flat, list_num_clusters):\n",
    "        grouped_skills=plot_skills(ax, k, df, str(k)+' Clusters')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # return grouped_skills\n",
    "\n",
    "# tuning the number of clusters for best result\n",
    "def plot_skills(ax, num_clusters, exploded_df, title, threshold_percentage=50):\n",
    "    top_skills = get_top_skills(num_clusters, exploded_df, threshold_percentage)\n",
    "    # plot them\n",
    "    sns.barplot(data=top_skills, x='most_freq_skill', y='count', errorbar=None, ax=ax)\n",
    "    ax.set_title(f'top skills of {title}')\n",
    "    ax.set_xlabel('skill name')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=75, ha='right')\n",
    "\n",
    "def plot_skills_by_title(df):\n",
    "    title_list = list(df['simplified_job_title'].unique())\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize = (16, 16))\n",
    "\n",
    "    for ax, title in zip(axes.flat, title_list):\n",
    "        df_title = df[df['simplified_job_title']==title].copy()\n",
    "        k = int(len(df_title['cleaned_required_skills'].unique())/5)\n",
    "        plot_skills(ax, k, df_title, title)\n",
    "    # plt.savefig('skill_by_title.png', dpi=300, bbox_inches='tight')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def add_qualifier(row):\n",
    "    if row == 'r':\n",
    "        return 'r language'\n",
    "    elif row == 'c++':\n",
    "        return 'c++ language'\n",
    "    elif ('skill' in row) or ('skills' in row):\n",
    "        return ' '.join(word for word in row.split() if word not in ['skill', 'skills'])\n",
    "    else:\n",
    "        return row\n",
    "    \n",
    "# Function to find the best match and group similar names using fuzzy match\n",
    "def skills_fuzzymatch(skill_list, threshold=80):\n",
    "    grouped_skills = []  # List to store the groups of similar skills\n",
    "\n",
    "    for skill in skill_list:\n",
    "        # Find if the skill is already grouped\n",
    "        matched = False\n",
    "        for group in grouped_skills:\n",
    "            # Compare with the first skill in each group\n",
    "            if fuzz.ratio(skill, group[0]) >= threshold:\n",
    "                group.append(skill)\n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        # If no match found, create a new group\n",
    "        if not matched:\n",
    "            grouped_skills.append([skill])\n",
    "\n",
    "    return grouped_skills\n",
    "\n",
    "def plot_fuzzy_match(df):\n",
    "    grouped_skills = df.groupby('grouped_skill').size().reset_index(name='count')\n",
    "    top_skills = grouped_skills.sort_values(by='count', ascending=False).head(20)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(data=top_skills, x='grouped_skill', y='count')\n",
    "    format_plot(ax, title='top skills', xlabel='skill name')\n",
    "    plt.savefig(\"skill_fuzzymatch.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    return top_skills\n",
    "\n",
    "# assign the grouped skills back to a new column in original DataFrame\n",
    "def assign_group(skill, grouped_skills):\n",
    "    for group in grouped_skills:\n",
    "        if skill in group:\n",
    "            return most_frequent_word(group)  # Return the first skill in the group as the representative\n",
    "    return skill  # If no group, return the skill itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Analysis Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function cleans the location names\n",
    "def clean_location_names(df):\n",
    "    location_mapping = {\n",
    "        'New York City Metropolitan Area': 'New York, NY',\n",
    "        'Albany, New York Metropolitan Area': 'Albany, NY',\n",
    "        'District of Columbia, United States': 'Washington, DC',\n",
    "        'San Francisco County, CA': 'San Francisco, CA',\n",
    "        'San Francisco Bay Area': 'San Francisco, CA',\n",
    "        'Los Angeles Metropolitan Area': 'Los Angeles, CA',\n",
    "        'Los Angeles County, CA': 'Los Angeles, CA',\n",
    "        'Dallas-Fort Worth Metroplex': 'Dallas, TX',\n",
    "        'Austin, Texas Metropolitan Area': 'Austin, TX',\n",
    "        'San Antonio, Texas Metropolitan Area': 'San Antonio, TX',\n",
    "        'Greater Philadelphia': 'Philadelphia, PA',\n",
    "        'Greater Seattle Area': 'Seattle, WA',\n",
    "        'Atlanta Metropolitan Area': 'Atlanta, GA',\n",
    "        'Annapolis Junction, MD': 'Annapolis, MD',\n",
    "        'Greater Minneapolis-St. Paul Area': 'Minneapolis, MN',\n",
    "        'Detroit Metropolitan Area': 'Detroit, MI',\n",
    "        'Charlotte Metro': 'Charlotte, NC',\n",
    "        'Green Bay, Wisconsin Metropolitan Area': 'Green Bay, WI'\n",
    "    }\n",
    "    \n",
    "    df['cleaned_location'] = df['location'].map(location_mapping).fillna(df['location'])\n",
    "    \n",
    "    return df['cleaned_location']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
