{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# AI\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Natural Language Processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from fuzzywuzzy import fuzz\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatgpt api\n",
    "load_dotenv('keys.env')\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# this function call the openai api, input the prompt and return the response in a string\n",
    "# string is cleaned to only return the python dictionary\n",
    "def transform(row, prompt):\n",
    "    # concat directions and the input text\n",
    "    prompt = f'{prompt}{row}'\n",
    "    # print(prompt)\n",
    "    retry = 0\n",
    "\n",
    "    # call api\n",
    "    while retry <= 10:\n",
    "        print(f\"Start API call at {datetime.now()}\")\n",
    "        try:\n",
    "            # Call API\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                timeout=60\n",
    "            )\n",
    "            content = str(response.choices[0].message.content)\n",
    "\n",
    "            # Handle 'null' and 'None' by replacing them with actual Python None\n",
    "            content = content.replace(\"null\", \"None\")\n",
    "\n",
    "            # Extract JSON-like content between first `{` and last `}`\n",
    "            start = content.find('{')\n",
    "            end = content.rfind('}')\n",
    "            if start != -1 and end != -1:\n",
    "                content = content[start:end+1]\n",
    "                # make sure it's a valid dictionary format that can be converted using eval\n",
    "                df_test = pd.DataFrame([[content]], columns=['test_content'])\n",
    "                df_test = df_test['test_content'].apply(eval)\n",
    "                print(f\"retrieved dictionary at {datetime.now()}\")\n",
    "                return content\n",
    "\n",
    "            else:\n",
    "                print(f\"No valid JSON-like content found in response at {datetime.now()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during API call or processing: {e}\")\n",
    "        \n",
    "        retry += 1\n",
    "        print(f\"Retrying {retry}...\")\n",
    "        time.sleep(4)\n",
    "\n",
    "    print(\"Failed to retrieve dictionary after maximum retries.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_plot(ax, title='', xlabel='', ylabel='', legend_title='', rotation=60):\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    if xlabel:\n",
    "        ax.set_xlabel(xlabel)\n",
    "    if ylabel:    \n",
    "        ax.set_ylabel(ylabel)\n",
    "    if legend_title:\n",
    "        ax.legend_.set_title(legend_title)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=rotation, ha='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all job with non-English title\n",
    "def remove_non_english_jobs(df):\n",
    "    # this function remove non-printable characters and replace non-ASC II characters to ASC II\n",
    "    def replace_non_ascii(s):\n",
    "        # remove non-printable characters\n",
    "        s = ''.join(ch for ch in s if ch.isprintable())\n",
    "        # Replace some special characters to space\n",
    "        s = s.replace(\"â€“\", \" \").replace(\"&\", \"and\").replace('/', ' ')\n",
    "        return s\n",
    "    \n",
    "    df2 = df.copy()\n",
    "    print(df2.columns)\n",
    "    df2['title'] = df2['title'].apply(replace_non_ascii)\n",
    "    return df2[~df2['title'].apply(contains_non_english)]\n",
    "\n",
    "# Return True if the text contains non-English characters, False if all English\n",
    "def contains_non_english(text):\n",
    "    return bool(re.search(r'[^\\x00-\\x7F]+', text))  # Matches any non-ASCII characters\n",
    "\n",
    "# remove unwanted rows by company names and job titles\n",
    "def reduce_rows(df):\n",
    "    # list of companies that only collect information, post fake posts, are offering coaching service instead of real jobs, and job board\n",
    "    companyName_to_drop = ['SynergisticIT', 'Outlier', 'Credible', 'HireMeFast LLC', 'Phoenix Recruitment', 'Jobs via Dice', 'Underdog.io', 'TekJobs', 'Henry Hire Solutions', 'Lifelancer', 'Global Technical Talent, an Inc. 5000 Company', 'HHS Careers', 'Jerry', 'Talentify.io', 'TELUS Digital AI Data Solutions']\n",
    "    # requires US citizenship and security clearance\n",
    "    cns = ['US Citizenship is required', 'security clearance']\n",
    "    title_to_keep = ['Intern', 'Data Analyst', 'Data Scientist', 'Business Intelligence']\n",
    "\n",
    "    # drop not legit companies\n",
    "    df = df[~df['companyName'].isin(companyName_to_drop)]\n",
    "    # remove rows contains citizenship and security clearance requirement\n",
    "    pattern = '|'.join(rf'\\b{word}\\b' for word in cns)\n",
    "    df = df[~df['description'].str.contains(pattern, na=False)] \n",
    "    # keep legit job titles\n",
    "    df = df[df['title'].str.contains('|'.join(title_to_keep), case=False, na=False)]\n",
    "    # remove duplicates by company name and title, keep the newest post\n",
    "    df = df.sort_values(by='publishedAt', ascending=False)\n",
    "    df = df.drop_duplicates(subset=['companyName', 'title'], keep='first')\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# condense complex job titles into a few well defined ones\n",
    "def simplify_title(row, simplify_title_list):\n",
    "    for title in simplify_title_list:\n",
    "        if title.lower() in row['title'].lower():\n",
    "            return title\n",
    "    return None  # Return None if not a match or if title is not a string\n",
    "\n",
    "# Convert dictionary column into new DataFrame columns\n",
    "def dict_to_cols(df, dict_column):\n",
    "    # Safely evaluate the dictionary column\n",
    "    # df[dict_column] = df[dict_column].apply(eval)\n",
    "    \n",
    "    # Normalize the dictionary column into separate columns\n",
    "    normalized_df = pd.json_normalize(df[dict_column])\n",
    "\n",
    "    # Replace 0 with None in the specified columns\n",
    "    normalized_df[['min_hourly_salary', 'max_hourly_salary']] = \\\n",
    "    normalized_df[['min_hourly_salary', 'max_hourly_salary']].replace(0, None)\n",
    "    normalized_df[['min_yearly_salary', 'max_yearly_salary']] = \\\n",
    "    normalized_df[['min_yearly_salary', 'max_yearly_salary']].replace(0, None)\n",
    "\n",
    "    # Replace None with 'not remote' in the 'is_remote' column\n",
    "    normalized_df['is_remote'] = normalized_df['is_remote'].fillna('not remote')\n",
    "    normalized_df['is_remote'] = normalized_df['is_remote'].replace('False', 'not remote')\n",
    "    normalized_df['is_remote'] = normalized_df['is_remote'].replace(False, 'not remote')\n",
    "    normalized_df['is_remote'] = normalized_df['is_remote'].replace('True', 'remote')\n",
    "    normalized_df['is_remote'] = normalized_df['is_remote'].replace(True, 'remote')\n",
    "    \n",
    "    # Combine the normalized columns with the original DataFrame (optional)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return pd.concat([df, normalized_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salary Analysis Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function cleans the salary data for further feature engineering and visualization\n",
    "# zeros are removed\n",
    "# The dataset has a salary column obtained by web scraping, and some salary from AI generated values\n",
    "# fill hourly and yearly salary if a salary column exist, otherwise keep the AI generated values\n",
    "def clean_salary(df, cols):\n",
    "    # remove zeros\n",
    "    df = df.replace(0, None)\n",
    "    # call parse_and_fill_salary to fill the hourly and yearly salary columns\n",
    "    df = df.apply(lambda row: parse_and_fill_salary(row), axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# this function parse the salary column and fill the hourly and yearly salary columns\n",
    "def parse_and_fill_salary(row):\n",
    "    # Function to extract the numeric values from salary strings\n",
    "    def extract_salary_value(salary_str):\n",
    "        numbers = re.findall(r'\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?', salary_str)\n",
    "        # Convert the numbers to float and return the list\n",
    "        return [float(num.replace(',', '')) for num in numbers]\n",
    "\n",
    "    # Checking if 'salary' is defined and hourly or yearly salary is missing\n",
    "    # if row['salary'] is not None:\n",
    "    if isinstance(row['salary'], str):\n",
    "        salary_value = extract_salary_value(row['salary'])\n",
    "        \n",
    "        # Case 1: If salary is hourly and min/max hourly salary is missing\n",
    "        if 'hr' in row['salary'] or 'hour' in row['salary']:\n",
    "            row['min_hourly_salary'] = min(salary_value)\n",
    "            row['max_hourly_salary'] = max(salary_value)\n",
    "            row['min_yearly_salary'] = None\n",
    "            row['max_yearly_salary'] = None\n",
    "        \n",
    "        # Case 2: If salary is monthly and min/max yearly salary is missing\n",
    "        elif 'mo' in row['salary'] or 'month' in row['salary']:\n",
    "            row['min_yearly_salary'] = min(salary_value)*12\n",
    "            row['max_yearly_salary'] = max(salary_value)*12\n",
    "            row['min_hourly_salary'] = None\n",
    "            row['max_hourly_salary'] = None\n",
    "        \n",
    "        # Case 3: If salary is yearly and min/max hourly salary is missing\n",
    "        elif 'yr' in row['salary'] or 'year' in row['salary']:\n",
    "            row['min_yearly_salary'] = min(salary_value)\n",
    "            row['max_yearly_salary'] = max(salary_value)\n",
    "            row['min_hourly_salary'] = None\n",
    "            row['max_hourly_salary'] = None\n",
    "            \n",
    "    return row\n",
    "\n",
    "# calculate average salary\n",
    "def calculate_average_salary(row, col_min, col_max):\n",
    "    # Check if both specified columns are not NaN\n",
    "    if pd.notna(row[col_min]) and pd.notna(row[col_max]):\n",
    "        return (row[col_min] + row[col_max]) / 2\n",
    "    # If the minimum column is NaN, return the maximum column value\n",
    "    elif pd.isna(row[col_min]) and pd.notna(row[col_max]):\n",
    "        return row[col_max]\n",
    "    # If the maximum column is NaN, return the minimum column value\n",
    "    elif pd.isna(row[col_max]) and pd.notna(row[col_min]):\n",
    "        return row[col_min]\n",
    "    else:\n",
    "        # If both are NaN, return NaN (or any custom value you'd prefer)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Skill Analysis Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means clustering\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize, remove stopwords, and lemmatize\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    if not words:\n",
    "        return None\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def KMeans_words(n, df):\n",
    "    df = df.copy()\n",
    "    # df['cleaned_text'] = df.apply(preprocess_text)\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(df)\n",
    "\n",
    "    # Fit KMeans clustering model\n",
    "    kmeans = KMeans(n_clusters=n, random_state=42)\n",
    "    df['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "    # grouped_words = df.groupby(['cluster', 'simplified_job_title'])['cleaned_text'].apply(list).reset_index()\n",
    "    # return grouped_words\n",
    "    return df['cluster']\n",
    "\n",
    "def most_frequent_word(text_list):\n",
    "    # Flatten the list of cleaned text into individual words\n",
    "    # all_words = \" \".join(text_list).split()\n",
    "    # Count the frequency of each word\n",
    "    # word_counts = Counter(all_words)\n",
    "    word_counts = Counter(text_list)\n",
    "    # Find the most common word\n",
    "    if word_counts:\n",
    "        return word_counts.most_common(1)[0][0]\n",
    "    return None\n",
    "\n",
    "def get_top_skills(num_clusters, exploded_df, threshold_percentage=50):\n",
    "    # clean the required skills again for clustering\n",
    "    exploded_df = exploded_df[exploded_df['cleaned_required_skills'] != 'etl'].copy()\n",
    "    exploded_df['cleaned_required_skills'] = exploded_df['cleaned_required_skills'].apply(add_qualifier)\n",
    "    exploded_df = exploded_df[exploded_df['cleaned_required_skills'].notna()].copy()\n",
    "    \n",
    "    # us k-means clustring to group similar skill names  \n",
    "    exploded_df['cluster'] = KMeans_words(n=num_clusters, df=exploded_df['cleaned_required_skills'])\n",
    "    \n",
    "    # calculate counts and standard names from the clusters\n",
    "    grouped_skills = exploded_df.groupby('cluster')['cleaned_required_skills'].apply(list).reset_index()\n",
    "    grouped_skills['count'] = grouped_skills['cleaned_required_skills'].apply(len)\n",
    "    grouped_skills['most_freq_skill'] = grouped_skills['cleaned_required_skills'].apply(most_frequent_word)\n",
    "    \n",
    "    # Calculate the frequency of the most frequent word in the cluster\n",
    "    def most_freq_word_count_percentage(skill_list, most_freq_skill):\n",
    "        return skill_list.count(most_freq_skill) / len(skill_list) * 100\n",
    "    \n",
    "    grouped_skills['most_freq_skill_percentage'] = grouped_skills.apply(\n",
    "        lambda row: most_freq_word_count_percentage(row['cleaned_required_skills'], row['most_freq_skill']),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Exclude clusters where the most frequent word count is below the threshold percentage\n",
    "    filtered_skills = grouped_skills[grouped_skills['most_freq_skill_percentage'] >= threshold_percentage]\n",
    "    \n",
    "    # get the top 20 skills\n",
    "    top_skills = filtered_skills.groupby('most_freq_skill')['count'].sum().reset_index()\n",
    "    top_skills = top_skills.sort_values(by='count', ascending=False).head(20)\n",
    "\n",
    "    return top_skills\n",
    "\n",
    "# test and plot 9 different number of clusters\n",
    "def tune_num_clusters(df):\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    l = len(df['cleaned_required_skills'].unique())\n",
    "    list_num_clusters = [int(l/10), int(l/9), int(l/8), int(l/7), int(l/6), int(l/5), int(l/4), int(l/3), int(l/2)]\n",
    "    for ax, k in zip(axes.flat, list_num_clusters):\n",
    "        grouped_skills=plot_skills(ax, k, df, str(k)+' Clusters')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # return grouped_skills\n",
    "\n",
    "# tuning the number of clusters for best result\n",
    "def plot_skills(ax, num_clusters, exploded_df, title, threshold_percentage=50):\n",
    "    top_skills = get_top_skills(num_clusters, exploded_df, threshold_percentage)\n",
    "    # plot them\n",
    "    sns.barplot(data=top_skills, x='most_freq_skill', y='count', errorbar=None, ax=ax)\n",
    "    ax.set_title(f'top skills of {title}')\n",
    "    ax.set_xlabel('skill name')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=75, ha='right')\n",
    "\n",
    "def plot_skills_by_title(df):\n",
    "    title_list = list(df['simplified_job_title'].unique())\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize = (16, 16))\n",
    "\n",
    "    for ax, title in zip(axes.flat, title_list):\n",
    "        df_title = df[df['simplified_job_title']==title].copy()\n",
    "        k = int(len(df_title['cleaned_required_skills'].unique())/5)\n",
    "        plot_skills(ax, k, df_title, title)\n",
    "    # plt.savefig('skill_by_title.png', dpi=300, bbox_inches='tight')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def add_qualifier(row):\n",
    "    if row == 'r':\n",
    "        return 'r language'\n",
    "    elif row == 'c++':\n",
    "        return 'c++ language'\n",
    "    elif ('skill' in row) or ('skills' in row):\n",
    "        return ' '.join(word for word in row.split() if word not in ['skill', 'skills'])\n",
    "    else:\n",
    "        return row\n",
    "    \n",
    "# Function to find the best match and group similar names using fuzzy match\n",
    "def skills_fuzzymatch(skill_list, threshold=80):\n",
    "    grouped_skills = []  # List to store the groups of similar skills\n",
    "\n",
    "    for skill in skill_list:\n",
    "        # Find if the skill is already grouped\n",
    "        matched = False\n",
    "        for group in grouped_skills:\n",
    "            # Compare with the first skill in each group\n",
    "            if fuzz.ratio(skill, group[0]) >= threshold:\n",
    "                group.append(skill)\n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        # If no match found, create a new group\n",
    "        if not matched:\n",
    "            grouped_skills.append([skill])\n",
    "\n",
    "    return grouped_skills\n",
    "\n",
    "def plot_fuzzy_match(df):\n",
    "    grouped_skills = df.groupby('grouped_skill').size().reset_index(name='count')\n",
    "    top_skills = grouped_skills.sort_values(by='count', ascending=False).head(20)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = sns.barplot(data=top_skills, x='grouped_skill', y='count')\n",
    "    format_plot(ax, title='top skills', xlabel='skill name')\n",
    "    plt.savefig(\"skill_fuzzymatch.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    return top_skills\n",
    "\n",
    "# assign the grouped skills back to a new column in original DataFrame\n",
    "def assign_group(skill, grouped_skills):\n",
    "    for group in grouped_skills:\n",
    "        if skill in group:\n",
    "            return most_frequent_word(group)  # Return the first skill in the group as the representative\n",
    "    return skill  # If no group, return the skill itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Analysis Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function cleans the location names\n",
    "def clean_location_names(df):\n",
    "    location_mapping = {\n",
    "        'New York City Metropolitan Area': 'New York, NY',\n",
    "        'Albany, New York Metropolitan Area': 'Albany, NY',\n",
    "        'District of Columbia, United States': 'Washington, DC',\n",
    "        'San Francisco County, CA': 'San Francisco, CA',\n",
    "        'San Francisco Bay Area': 'San Francisco, CA',\n",
    "        'Los Angeles Metropolitan Area': 'Los Angeles, CA',\n",
    "        'Los Angeles County, CA': 'Los Angeles, CA',\n",
    "        'Dallas-Fort Worth Metroplex': 'Dallas, TX',\n",
    "        'Austin, Texas Metropolitan Area': 'Austin, TX',\n",
    "        'San Antonio, Texas Metropolitan Area': 'San Antonio, TX',\n",
    "        'Greater Philadelphia': 'Philadelphia, PA',\n",
    "        'Greater Seattle Area': 'Seattle, WA',\n",
    "        'Atlanta Metropolitan Area': 'Atlanta, GA',\n",
    "        'Annapolis Junction, MD': 'Annapolis, MD',\n",
    "        'Greater Minneapolis-St. Paul Area': 'Minneapolis, MN',\n",
    "        'Detroit Metropolitan Area': 'Detroit, MI',\n",
    "        'Charlotte Metro': 'Charlotte, NC',\n",
    "        'Green Bay, Wisconsin Metropolitan Area': 'Green Bay, WI'\n",
    "    }\n",
    "    \n",
    "    df['cleaned_location'] = df['location'].map(location_mapping).fillna(df['location'])\n",
    "    \n",
    "    return df['cleaned_location']\n",
    "\n",
    "def appliable_pos(df):\n",
    "    # add two new columns\n",
    "    df['applied'] = None\n",
    "    df['resume_ver'] = None\n",
    "    df['reason_not_apply'] = None\n",
    "\n",
    "    cols_to_keep = ['applyUrl', 'companyName', 'contractType', 'experienceLevel', 'jobUrl', 'location',\n",
    "       'posterFullName', 'posterProfileUrl', 'publishedAt', 'salary', 'title', 'workType', 'dict_ai', 'min_years_of_experience', 'is_remote', 'applied', 'resume_ver', 'reason_not_apply']\n",
    "\n",
    "    # remove intern\n",
    "    df = df[df['simplified_job_title']!='Intern']\n",
    "\n",
    "    # get remote jobs\n",
    "    df_remote = df[(df['is_remote']=='remote') | (df['location']=='United States')]\n",
    "\n",
    "    # define local key words\n",
    "    location_list = [', WI', 'Wisconsin', 'Milwaukee', 'Chicago']\n",
    "    pattern = '|'.join(location_list)\n",
    "\n",
    "    # match location names with local key words\n",
    "    df_local = df[df['location'].str.contains(pattern, case=False, na=False)]\n",
    "    df_apply = pd.concat([df_remote, df_local], axis=0).reset_index(drop=True)\n",
    "    return df_apply[cols_to_keep]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
