{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from utils2 import refine, reduce_rows, remove_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Learn/projects/data/job_data/mine/linkedin_jobs_data_02042025.csv\n",
      "(691, 14)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 691 entries, 0 to 690\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   job_id           691 non-null    int64 \n",
      " 1   job_title        691 non-null    object\n",
      " 2   job_url          691 non-null    object\n",
      " 3   job_location     691 non-null    object\n",
      " 4   company_name     691 non-null    object\n",
      " 5   company_url      691 non-null    object\n",
      " 6   posted_date      689 non-null    object\n",
      " 7   workplace        691 non-null    object\n",
      " 8   salary           318 non-null    object\n",
      " 9   description      691 non-null    object\n",
      " 10  seniority_level  691 non-null    object\n",
      " 11  employment_type  691 non-null    object\n",
      " 12  job_function     689 non-null    object\n",
      " 13  industries       689 non-null    object\n",
      "dtypes: int64(1), object(13)\n",
      "memory usage: 75.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "now = datetime.now()\n",
    "date = now.strftime('%m%d%Y')\n",
    "path = 'D:/Learn/projects/data/job_data/mine/'\n",
    "data_file = f'{path}linkedin_jobs_data_{date}.csv'\n",
    "print(data_file)\n",
    "\n",
    "# df_remote = pd.read_csv(f'{path}dataset_linkedin-jobs-scraper---remote_2025-01-29_01-01-44-745.csv')\n",
    "# df_remote['workPlace'] = 'remote'\n",
    "# df_hybrid = pd.read_csv(f'{path}dataset_linkedin-jobs-scraper---hybrid_2025-01-29_01-02-00-602.csv')\n",
    "# df_hybrid['workPlace'] = 'hybrid'\n",
    "# df_onsite = pd.read_csv(f'{path}dataset_linkedin-jobs-scraper---onsite_2025-01-29_01-03-37-177.csv')\n",
    "# df_onsite['workPlace'] = 'onsite'\n",
    "# df = pd.concat([df_remote, df_hybrid], axis=0, ignore_index=True)\n",
    "# df = pd.concat([df, df_onsite], axis=0, ignore_index=True)\n",
    "df = pd.read_csv(data_file)\n",
    "print(df.shape)\n",
    "df.info(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "droped empty rows, remaining 691\n",
      "droped duplicates, remaining 665\n",
      "Company dropped:  135            Applicantz\n",
      "157       RemoteWorker CA\n",
      "329      GalaxE.Solutions\n",
      "371    American Unit, Inc\n",
      "Name: company_name, dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 195 entries, 0 to 490\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   job_id           195 non-null    int64 \n",
      " 1   job_title        195 non-null    object\n",
      " 2   job_url          195 non-null    object\n",
      " 3   job_location     195 non-null    object\n",
      " 4   company_name     195 non-null    object\n",
      " 5   company_url      195 non-null    object\n",
      " 6   posted_date      195 non-null    object\n",
      " 7   workplace        195 non-null    object\n",
      " 8   salary           71 non-null     object\n",
      " 9   seniority_level  195 non-null    object\n",
      " 10  employment_type  195 non-null    object\n",
      " 11  job_function     195 non-null    object\n",
      " 12  industries       194 non-null    object\n",
      "dtypes: int64(1), object(12)\n",
      "memory usage: 21.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# remove empty and duplicated data\n",
    "df1 = refine(df)\n",
    "# df1.to_csv('jobs.csv', index=False)\n",
    "df2 = reduce_rows(df1)\n",
    "df3 = remove_companies(df2, path)\n",
    "df_to_apply = df2[(df2['workplace']=='remote') | (df2['job_location'].str.contains('WI', na=False)) | (df2['job_location'].str.contains('Milwaukee', na=False))].drop(columns=['description'])\n",
    "df_to_apply.to_csv(f'{path}to_apply_{date}.csv', index=False)\n",
    "df_to_apply.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
